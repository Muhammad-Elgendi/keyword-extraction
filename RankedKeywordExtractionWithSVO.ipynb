{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RankedKeywordExtractionWithSVO.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoGRXUO0QGEyFy7VdFEzmO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"El3kk3YLYOFn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552259616,"user_tz":-120,"elapsed":677,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# apply RAKE algorithm on text and return list of keywords\n","def applyRake(text):\n","  # install rake_nltk\n","  !pip install rake-nltk\n","\n","  import nltk\n","  nltk.download('stopwords')\n","\n","  from rake_nltk import Rake\n","\n","  r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n","\n","  r.extract_keywords_from_text(text)\n","  return r.get_ranked_phrases()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gqlulgq9WWTL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552263259,"user_tz":-120,"elapsed":2162,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# subject - verb - object extraction using spacy\n","\n","import en_core_web_sm\n","# use spacy small model\n","nlp = en_core_web_sm.load()\n","\n","# dependency markers for subjects\n","SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n","# dependency markers for objects\n","OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n","# POS tags that will break adjoining items\n","BREAKER_POS = {\"CCONJ\", \"VERB\"}\n","# words that are negations\n","NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n","\n","\n","# does dependency set contain any coordinating conjunctions?\n","def contains_conj(depSet):\n","    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n","           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n","\n","\n","# get subs joined by conjunctions\n","def _get_subs_from_conjunctions(subs):\n","    more_subs = []\n","    for sub in subs:\n","        # rights is a generator\n","        rights = list(sub.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if contains_conj(rightDeps):\n","            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n","            if len(more_subs) > 0:\n","                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n","    return more_subs\n","\n","\n","# get objects joined by conjunctions\n","def _get_objs_from_conjunctions(objs):\n","    more_objs = []\n","    for obj in objs:\n","        # rights is a generator\n","        rights = list(obj.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if contains_conj(rightDeps):\n","            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n","            if len(more_objs) > 0:\n","                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n","    return more_objs\n","\n","\n","# find sub dependencies\n","def _find_subs(tok):\n","    head = tok.head\n","    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n","        head = head.head\n","    if head.pos_ == \"VERB\":\n","        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n","        if len(subs) > 0:\n","            verb_negated = _is_negated(head)\n","            subs.extend(_get_subs_from_conjunctions(subs))\n","            return subs, verb_negated\n","        elif head.head != head:\n","            return _find_subs(head)\n","    elif head.pos_ == \"NOUN\":\n","        return [head], _is_negated(tok)\n","    return [], False\n","\n","\n","# is the tok set's left or right negated?\n","def _is_negated(tok):\n","    parts = list(tok.lefts) + list(tok.rights)\n","    for dep in parts:\n","        if dep.lower_ in NEGATIONS:\n","            return True\n","    return False\n","\n","\n","# get all the verbs on tokens with negation marker\n","def _find_svs(tokens):\n","    svs = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n","    for v in verbs:\n","        subs, verbNegated = _get_all_subs(v)\n","        if len(subs) > 0:\n","            for sub in subs:\n","                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n","    return svs\n","\n","\n","# get grammatical objects for a given set of dependencies (including passive sentences)\n","def _get_objs_from_prepositions(deps, is_pas):\n","    objs = []\n","    for dep in deps:\n","        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n","            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or\n","                         (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n","                         (is_pas and tok.dep_ == 'pobj')])\n","    return objs\n","\n","\n","# get objects from the dependencies using the attribute dependency\n","def _get_objs_from_attrs(deps, is_pas):\n","    for dep in deps:\n","        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n","            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n","            if len(verbs) > 0:\n","                for v in verbs:\n","                    rights = list(v.rights)\n","                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n","                    if len(objs) > 0:\n","                        return v, objs\n","    return None, None\n","\n","\n","# xcomp; open complement - verb has no suject\n","def _get_obj_from_xcomp(deps, is_pas):\n","    for dep in deps:\n","        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n","            v = dep\n","            rights = list(v.rights)\n","            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n","            if len(objs) > 0:\n","                return v, objs\n","    return None, None\n","\n","\n","# get all functional subjects adjacent to the verb passed in\n","def _get_all_subs(v):\n","    verb_negated = _is_negated(v)\n","    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n","    if len(subs) > 0:\n","        subs.extend(_get_subs_from_conjunctions(subs))\n","    else:\n","        foundSubs, verb_negated = _find_subs(v)\n","        subs.extend(foundSubs)\n","    return subs, verb_negated\n","\n","\n","# is the token a verb?  (excluding auxiliary verbs)\n","def _is_non_aux_verb(tok):\n","    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n","\n","\n","# return the verb to the right of this verb in a CCONJ relationship if applicable\n","# returns a tuple, first part True|False and second part the modified verb if True\n","def _right_of_verb_is_conj_verb(v):\n","    # rights is a generator\n","    rights = list(v.rights)\n","\n","    # VERB CCONJ VERB (e.g. he beat and hurt me)\n","    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n","        for tok in rights[1:]:\n","            if _is_non_aux_verb(tok):\n","                return True, tok\n","\n","    return False, v\n","\n","\n","# get all objects for an active/passive sentence\n","def _get_all_objs(v, is_pas):\n","    # rights is a generator\n","    rights = list(v.rights)\n","\n","    objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n","    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n","\n","    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n","    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n","    #    objs.extend(potentialNewObjs)\n","    #    v = potentialNewVerb\n","\n","    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n","    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n","        objs.extend(potential_new_objs)\n","        v = potential_new_verb\n","    if len(objs) > 0:\n","        objs.extend(_get_objs_from_conjunctions(objs))\n","    return v, objs\n","\n","\n","# return true if the sentence is passive - at he moment a sentence is assumed passive if it has an auxpass verb\n","def _is_passive(tokens):\n","    for tok in tokens:\n","        if tok.dep_ == \"auxpass\":\n","            return True\n","    return False\n","\n","\n","# resolve a 'that' where/if appropriate\n","def _get_that_resolution(toks):\n","    for tok in toks:\n","        if 'that' in [t.orth_ for t in tok.lefts]:\n","            return tok.head\n","    return toks\n","\n","\n","# simple stemmer using lemmas\n","def _get_lemma(word: str):\n","    tokens = nlp(word)\n","    if len(tokens) == 1:\n","        return tokens[0].lemma_\n","    return word\n","\n","\n","# print information for displaying all kinds of things of the parse tree\n","def printDeps(toks):\n","    for tok in toks:\n","        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n","\n","\n","# expand an obj / subj np using its chunk\n","def expand(item, tokens, visited):\n","    if item.lower_ == 'that':\n","        item = _get_that_resolution(tokens)\n","\n","    parts = []\n","\n","    if hasattr(item, 'lefts'):\n","        for part in item.lefts:\n","            if part.pos_ in BREAKER_POS:\n","                break\n","            if not part.lower_ in NEGATIONS:\n","                parts.append(part)\n","\n","    parts.append(item)\n","\n","    if hasattr(item, 'rights'):\n","        for part in item.rights:\n","            if part.pos_ in BREAKER_POS:\n","                break\n","            if not part.lower_ in NEGATIONS:\n","                parts.append(part)\n","\n","    if hasattr(parts[-1], 'rights'):\n","        for item2 in parts[-1].rights:\n","            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n","                if item2.i not in visited:\n","                    visited.add(item2.i)\n","                    parts.extend(expand(item2, tokens, visited))\n","            break\n","\n","    return parts\n","\n","\n","# convert a list of tokens to a string\n","def to_str(tokens):\n","    return ' '.join([item.text for item in tokens])\n","\n","\n","# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n","def findSVOs(tokens):\n","    svos = []\n","    is_pas = _is_passive(tokens)\n","    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)]\n","    visited = set()  # recursion detection\n","    for v in verbs:\n","        subs, verbNegated = _get_all_subs(v)\n","        # hopefully there are subs, if not, don't examine this verb any longer\n","        if len(subs) > 0:\n","            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n","            if isConjVerb:\n","                v2, objs = _get_all_objs(conjV, is_pas)\n","                for sub in subs:\n","                    for obj in objs:\n","                        objNegated = _is_negated(obj)\n","                        if is_pas:  # reverse object / subject for passive\n","                            svos.append((to_str(expand(obj, tokens, visited)),\n","                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n","                            svos.append((to_str(expand(obj, tokens, visited)),\n","                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n","                        else:\n","                            svos.append((to_str(expand(sub, tokens, visited)),\n","                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n","                            svos.append((to_str(expand(sub, tokens, visited)),\n","                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n","            else:\n","                v, objs = _get_all_objs(v, is_pas)\n","                for sub in subs:\n","                    for obj in objs:\n","                        objNegated = _is_negated(obj)\n","                        if is_pas:  # reverse object / subject for passive\n","                            svos.append((to_str(expand(obj, tokens, visited)),\n","                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n","                        else:\n","                            svos.append((to_str(expand(sub, tokens, visited)),\n","                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n","    return svos"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nIuotRHYhm5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552272478,"user_tz":-120,"elapsed":855,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# Preprocess text and returns list of sentences\n","def preprocess(text):\n","  import nltk\n","  # apply sentence tokenization\n","  sents = []\n","  sents = nltk.sent_tokenize(text)\n","\n","  # remove punctuation\n","  import string \n","  # remove punctuation dictionary\n","  remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","  filteredSents = []\n","  for sent in sents:\n","    filteredSents.append(sent.translate(remove_punct_dict))\n","  return filteredSents"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2xhbkPAZvh7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552276447,"user_tz":-120,"elapsed":992,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# takes a list of sentences and returns a list of lists\n","# each sentence is a list which has tuples that represent\n","# subject , verb , object \n","def extractSVOFromSents(sentences):\n","  # extract subject verb object from keywords\n","  svo = []\n","  for sent in sentences:\n","    tokens = nlp(sent)\n","    svos = findSVOs(tokens)\n","    svo.append(svos)\n","  return svo  "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxllIJrmbIRh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552278951,"user_tz":-120,"elapsed":855,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# takes output from extractSVOFromSents() and \n","# return a list of sentences without subjects\n","def buildSentsFromSVO(svo):\n","  # filter svos\n","  sentsOfSVO = []\n","  for listOfSentences in svo:\n","    for sentence in listOfSentences:    \n","      subject,verb,obj = sentence\n","      sentsOfSVO.append(verb+\" \"+obj)\n","  return sentsOfSVO"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfVrQTOobwYv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552354036,"user_tz":-120,"elapsed":928,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# textRank implementation\n","import re\n","import numpy as np\n","from nltk import sent_tokenize, word_tokenize\n","from pprint import pprint\n","from nltk.cluster.util import cosine_distance\n","\n","MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n","\n","def normalize_whitespace(text):\n","    \"\"\"\n","    Translates multiple whitespace into single space character.\n","    If there is at least one new line character chunk is replaced\n","    by single LF (Unix new line) character.\n","    \"\"\"\n","    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n","\n","\n","def _replace_whitespace(match):\n","    text = match.group()\n","\n","    if \"\\n\" in text or \"\\r\" in text:\n","        return \"\\n\"\n","    else:\n","        return \" \"\n","\n","\n","def is_blank(string):\n","    \"\"\"\n","    Returns `True` if string contains only white-space characters\n","    or is empty. Otherwise `False` is returned.\n","    \"\"\"\n","    return not string or string.isspace()\n","\n","\n","def get_symmetric_matrix(matrix):\n","    \"\"\"\n","    Get Symmetric matrix\n","    :param matrix:\n","    :return: matrix\n","    \"\"\"\n","    return matrix + matrix.T - np.diag(matrix.diagonal())\n","\n","\n","def core_cosine_similarity(vector1, vector2):\n","    \"\"\"\n","    measure cosine similarity between two vectors\n","    :param vector1:\n","    :param vector2:\n","    :return: 0 < cosine similarity value < 1\n","    \"\"\"\n","    return 1 - cosine_distance(vector1, vector2)\n","\n","\n","'''\n","Note: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n","'''\n","\n","\n","class TextRank4Sentences():\n","    def __init__(self):\n","        self.damping = 0.85  # damping coefficient, usually is .85\n","        self.min_diff = 1e-5  # convergence threshold\n","        self.steps = 100  # iteration steps\n","        self.text_str = None\n","        self.sentences = None\n","        self.pr_vector = None\n","\n","    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n","        if stopwords is None:\n","            stopwords = []\n","\n","        sent1 = [w.lower() for w in sent1]\n","        sent2 = [w.lower() for w in sent2]\n","\n","        all_words = list(set(sent1 + sent2))\n","\n","        vector1 = [0] * len(all_words)\n","        vector2 = [0] * len(all_words)\n","\n","        # build the vector for the first sentence\n","        for w in sent1:\n","            if w in stopwords:\n","                continue\n","            vector1[all_words.index(w)] += 1\n","\n","        # build the vector for the second sentence\n","        for w in sent2:\n","            if w in stopwords:\n","                continue\n","            vector2[all_words.index(w)] += 1\n","\n","        return core_cosine_similarity(vector1, vector2)\n","\n","    def _build_similarity_matrix(self, sentences, stopwords=None):\n","        # create an empty similarity matrix\n","        sm = np.zeros([len(sentences), len(sentences)])\n","\n","        for idx1 in range(len(sentences)):\n","            for idx2 in range(len(sentences)):\n","                if idx1 == idx2:\n","                    continue\n","\n","                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n","\n","        # Get Symmeric matrix\n","        sm = get_symmetric_matrix(sm)\n","\n","        # Normalize matrix by column\n","        norm = np.sum(sm, axis=0)\n","        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n","\n","        return sm_norm\n","\n","    def _run_page_rank(self, similarity_matrix):\n","\n","        pr_vector = np.array([1] * len(similarity_matrix))\n","\n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n","            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr_vector)\n","\n","        return pr_vector\n","\n","    def _get_sentence(self, index):\n","\n","        try:\n","            return self.sentences[index]\n","        except IndexError:\n","            return \"\"\n","\n","    def get_top_sentences(self, number=0):\n","\n","        sorted_sent = []\n","        if self.pr_vector is not None:\n","\n","            sorted_pr = np.argsort(self.pr_vector)\n","            sorted_pr = list(sorted_pr)\n","            sorted_pr.reverse()\n","        \n","            if number == 0 :\n","                number = len(sorted_pr)\n","\n","            for index in range(number):\n","                # print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n","                sent = self.sentences[sorted_pr[index]]\n","                sent = normalize_whitespace(sent)\n","                sorted_sent.append([sent,self.pr_vector[sorted_pr[index]]])\n","            \n","\n","        return sorted_sent\n","\n","    def analyze(self, sentences, stop_words=None):\n","        self.sentences = sentences\n","\n","        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n","\n","        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n","\n","        self.pr_vector = self._run_page_rank(similarity_matrix)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"msHEa3dQc2U-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592552361364,"user_tz":-120,"elapsed":1000,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}}},"source":["# get a list of top keywords out of text\n","def getTopKeywords(text):\n","  keywords = applyRake(text)\n","  sents = preprocess(text)\n","  svo = extractSVOFromSents(sents)\n","  sentsOfSVO = buildSentsFromSVO(svo)\n","  # combine RAKE keywords with SVOs\n","  keywords.extend(sentsOfSVO)\n","  # remove duplicates from keywords\n","  keywords = list(dict.fromkeys(keywords))\n","  # apply text rank\n","  sentenceRanker = TextRank4Sentences()\n","  sentenceRanker.analyze(keywords)\n","  top_keywords = sentenceRanker.get_top_sentences()\n","  # filter top keywords\n","  filtered_keywords = []\n","  for keyword in top_keywords:\n","    if len(keyword[0].split()) > 1 and keyword[1] > 0.45 and len(keyword[0]) > 3:\n","      filtered_keywords.append([keyword[0],keyword[1]]) \n","  return filtered_keywords"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HvFcg_LaT5K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":971},"executionInfo":{"status":"ok","timestamp":1592552371315,"user_tz":-120,"elapsed":5478,"user":{"displayName":"Muhammad Elgendi","photoUrl":"","userId":"10560336987985645303"}},"outputId":"a57a26aa-b6b7-45e4-f0a9-4567d0e3db59"},"source":["# usage\n","string = '''I am trying to build something durable, I am trying to retire my parenets, what they are willing to admit and what they are doing under hood. I am trying to be recognized, accumulate capital. We can hit the nerve under the hood. Self actualization comes after basic things solved. You cant be philanthropic. you have to help first yourself before helping others. We need to solve first order problems. ?? They want to build and sell business. They understand the investmet game well. \\nGet thank you notes from customers, create a few jobs, sponsor a few picnics, sell for a cool 5-10x rev, dabble with some charity, and feel at least 1/10th the way Warren feels when he dances off to work.  \\nBuild products that people find useful.\\nDo meaningful and interesting work. \\nFoster a great culture of talented people and a fun working environment.  \\nCreate jobs and contribute to the community. \\nEventually sell for 5-10x revenue or 15-30x EBITDA without bringing on a load of investors.\\n\\nEventually push forth a charitable agenda. \\nLess churn, less CAC\\nDo not give away equity\\nIf you are trying to raise your next round, stop and stick around\\n7 Step Process to Turn Your SaaS Business into Growth Machine If You Don’t Want to Give Up Equity\\n\\nAsk the prospect - “If you could fix one problem in your life such that by fixing it, everything else would be irrelevant, what would that be?”\\n“What is screaming at you? Something you can’t ignore. Something you think about the moment you wake up?”',\n"," 'Notes: What these people want at the end of the day. We are not just B2B, B2C...we are going into customer world. These are our heros. These are our guys, without them we are nothing. We are not passing judgement, but we are scientist. We acknowledge this person as much as humanly possible. They will tell you these things when you work with them. When they are under stress what they do say. We need to get 1m in less than a year as personal desire. \\nBuy the house of their dreams\\nBe mentioned in alumni publications \\nDrive a Porsche or Tesla\\nDonate money to causes\\nMake their spouse, parents, children, and close friends proud\\nSend kids to private school\\nProudly talk of their success at Thanksgiving with family and friends\\nBe a model citizen; successful, kind, intelligent, generous, and humble\\nHave the financial freedom to travel the world with their kids\\nBuy your wife a new car'''\n","\n","getTopKeywords(string)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: rake-nltk in /usr/local/lib/python3.6/dist-packages (1.0.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[['hit the nerve', 1.6970595408339264],\n"," ['travel the financial freedom', 1.6434831382341968],\n"," ['understand the investmet game', 1.6075978040249281],\n"," ['mention the house of their dreams\\n', 1.4444156452638794],\n"," ['tell these things', 1.389402560525686],\n"," ['!give equity\\n', 1.3454783712681637],\n"," ['people want', 1.3033697763297756],\n"," ['sell business', 1.301052613582438],\n"," ['?” “', 1.2724996473002417],\n"," ['solve order problems', 1.2666354758946907],\n"," ['raise your next round stop', 1.255],\n"," ['eventually sell', 1.251883850658229],\n"," ['give Equity\\n', 1.2315863756768772],\n"," ['kids buy', 1.221296048809605],\n"," ['build something durable', 1.1888804377556572],\n"," ['retire my parenets', 1.1275],\n"," ['acknowledge this person', 1.1275],\n"," ['create jobs', 1.1275],\n"," ['solve first order problems', 1.1275],\n"," ['next round', 1.1006201256462531],\n"," ['give away equity', 1.0988184018738947],\n"," ['close friends proud send kids', 1.0558744456325178],\n"," ['cool 5', 1.0],\n"," ['customer world', 1.0],\n"," ['interesting work', 1.0],\n"," ['10x rev', 1.0],\n"," ['10x revenue', 1.0],\n"," ['get 1m', 1.0],\n"," ['get thank', 1.0],\n"," ['30x ebitda without bringing', 1.0],\n"," ['fix one problem', 1.0],\n"," ['everything else would', 1.0],\n"," ['could fix one problem', 1.0],\n"," ['build products', 0.995202321065977],\n"," ['talented people', 0.9654577492232516],\n"," ['less churn', 0.9633889821661666],\n"," ['less cac', 0.9633889821661666],\n"," ['!passing judgement', 0.9318553611101092],\n"," ['tell you', 0.8824546694143958],\n"," ['people find useful', 0.8768503421599131],\n"," ['financial freedom', 0.8312891480214479],\n"," ['passing judgement', 0.8146183059958236],\n"," ['basic things solved', 0.7887338627786014],\n"," ['investmet game well', 0.7438314868380708],\n"," ['equity ask', 0.6792171347174639],\n"," ['help first', 0.6058645241053096],\n"," ['saas business', 0.5702680987067659],\n"," ['eventually push forth', 0.5096004105439522]]"]},"metadata":{"tags":[]},"execution_count":22}]}]}